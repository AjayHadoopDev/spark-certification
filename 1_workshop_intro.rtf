{\rtf1\ansi\ansicpg1252\cocoartf1265\cocoasubrtf190
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red0\green61\blue204;\red0\green45\blue153;\red73\green0\blue0;
}
\margl1440\margr1440\vieww16260\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 \
Date:  Feb , 2016\
\
Documents: https://databricks.com/spark/developer-resources \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 Reference:   http://training.databricks.com/workshop/itas_workshop.pdf\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 \

\b Workshop Notes :\

\b0 \
Key points to start with , more to follow later on \'85..\
\
>> 
\b Fast Data Sharing
\b0 \
\
>> 
\b Lazy DAG Evaluation
\b0 \
\
>> 
\b Functional Programming
\b0  (sticking to original MR Papers)\
\
>> 
\b Less Expensive Shuffles
\b0 \
\
>> 
\b No Synchronization Barrier
\b0  - like Hadoop Jobs (every Job phase has no idea whats the succeeding / preceding task)\
\

\b Code Snippets :\
\
\cf2 $ code/spark-training/spark \cf0 \
\
\'97 using python\
\cf3 $ spark-1.3.0-bin-hadoop2.4/bin \
$ pyspark\cf0 \

\b0 \

\b \'97 using scala
\b0 \
$ brew install sbt\
$ code/spark/spark-training/spark\
$../sbt/sbt assembly\
\
\
##1\
val data = sc.parallelize(1 to 10000) => ParallelCollectionRDD\
\
>> Find all data less than 10\
val filteredRDD = data.filter(_ < 10).collect() || _ is current element \
\
##2\
val lines = sc.paralleliz(List("INFO - Tomcat shines on a sunny morning","ERROR mysql unauthorized exception","ERROR php syntax error"))\
val errors = lines.filter(_.startsWith("ERROR"))\
\

\b // split tokens -> parent to children RDD
\b0 \
val messageTokens = errors.map(_.split(" ")).map(p => p(1)) // take the second column\
\

\b // keep in cache  after the first action
\b0 \
messageTokens.cache()\
\
val mysqlMessages = messageTokens.filter(_.contains("mysql")).count()\
\

\b // uses cached messageTokens
\b0 \
val javaMessages = messageTokens.filter(_.contaims(\'93java\'94)).count()\
\
\'97\'97> \
\

\b Basic Example
\b0 \
\
val fileRDD1 = sc.textFile(\'93ERROR_DATA.txt")\
/.flatMap(_.split(" "))\
/.filter(_.contains("Spark"))\
/.map(w => (w,1))\
/.reduceByKey(_+_)\
\
val fileRDD2 = sc.textFile(\'93ORIGINAL_DATA.txt\'94).flatMap(_.split(" ")).filter(_.contains("Spark")).map(w => (w,1)).reduceByKey(_+_)\
\
fileRDD1.join(fileRDD2).collect()\
\
res18: Array[(String, (Int, Int))] = Array((Spark,(159,18)))\
\

\b Concepts :
\b0 \
\
1. Driver - first serializes the lineage DAG - and sends it to every Worker Node \
     -> also sends out the tasks to execute\
2. Every Worker reads a partition of RDD for each HDFS block and cache the message locally\
3. Performs the count locally and sends back the result to Driver - in parallel\
4. Driver aggregates the counts \
\
DriverManager \'97> SparkContext  \
- master connects to the 'Cluster Manager' to allocate resources\
- master acquires Executors on cluster nodes (Worker  can run multiple Executor Jvm processes) \
- master sends App Code (Closure Serialization) to the Executors \
- master sends Tasks for the Executor to run ...\
\
prefer re-computation of transformation to leverage memory (part that got lost) as opposed to duplication of data.\
\
Note the difference between Map and FlatMap >>\
\
map(l => l.split(" ")) >>> Array[Array[String]]\
flatMap(l => l.split(" ")) >>> [Array[String]\
\

\b Quick recap of scala and python :\

\b0 \
\'97 In Scala \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 val pair = (a,b)\
pair._1\
pair._2\
\
\'97 In Python\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural
\cf0 pair = (a,b)\
pair[0]\
pair[1]\
\
Tuple2 pair = new Tuple2(a,b)\
\
/////////\
\

\b Running Sample Code :
\b0 \
\

\b >>   SparkPi \

\b0 \

\b \cf3 ./bin/run-example SparkPi 2 local --conf spark.port.maxRetries=100\

\b0 \cf0 \
\cf4 val count = sc.parallelize(1 to n, 2) => ParallelCollectionRDD\
 	 |.map\{i => val x = 2*random - 1  => transformation :: MapPartitionsRDD\
     | val y = 2*random - 1\
     | if (x*x + y*y < 1) 1 else 0\
     | \}.reduce(_+_) => action\
\
val rdd1 = sc.parallelize(1 to n, 2).map\{i => val x = 2*random - 1  \
     | val y = 2*random - 1\
     | if (x*x + y*y < 1) 1 else 0\
     | \}\cf0 \
\

\b >> Spark Streaming - Twitter Algebird  -> Count-Min Sketch Algo : count Frequencies
\b0 \
\

\b \cf2 ./bin/spark-submit --class "org....TwitterAlgebirdCMS"     --master "local[*]" lib/spark-example-*.jar\

\b0 \cf0 \

\b >> Network Word Count
\b0 \
\

\b \cf2 Python >> ./bin/spark-submit nwc.py localhost 9999\
Scala >>  ./bin/run-example org.apache.spark.examples.streaming.NetworkWordCount localhost 9999 \
\
\cf0 >> Streaming Word Count\

\b0 \cf3 \
scala> val ssc = new StreamingContext(sc, Seconds(2))\
ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@170937fe\
\
scala> val line = ssc.socketTextStream("localhost", 9999)\
line: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@571b4bf6\
\
scala> val words = line.flatMap(_.split(" "))\
words: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.FlatMappedDStream@15eaa2ce\
\
scala> val wordCounts = words.map(w => (w,1)).reduceByKey(_+_)\
wordCounts: org.apache.spark.streaming.dstream.DStream[(String, Int)] = org.apache.spark.streaming.dstream.ShuffledDStream@2a59a8fe\
\
scala> wordCounts.print()\
\
scala> ssc.start()\
\

\b \cf0 >> more exercises
\b0 \cf3 \
\
https://github.com/ceteri/intro_spark/tree/master/bikeshare}