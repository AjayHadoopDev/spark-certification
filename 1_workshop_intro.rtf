Date:  Feb , 2016\
Documents: https://databricks.com/spark/developer-resources \

Exam Workshop Notes :\

Key points to remember ...  
Fast Data Sharing
Lazy DAG Evaluation
Functional Programming
Less Expensive Shuffles
No Synchronization Barrier
 - like Hadoop Jobs (every Job phase has no idea whats the succeeding / preceding task)\

Code Snippets:
$ code/spark-training/spark \cf0 \

using python:
$ spark-1.3.0-bin-hadoop2.4/bin \
$ pyspark\

using scala
$ brew install sbt\
$ code/spark/spark-training/spark\
$../sbt/sbt assembly\
\

val data = sc.parallelize(1 to 10000) => ParallelCollectionRDD\
\
>> Find all data less than 10\
val filteredRDD = data.filter(_ < 10).collect() || _ is current element \
\
##2\
val lines = sc.paralleliz(List("INFO - Tomcat shines on a sunny morning","ERROR mysql unauthorized exception","ERROR php syntax error"))\
val errors = lines.filter(_.startsWith("ERROR"))\
\

\b // split tokens -> parent to children RDD
\b0 \
val messageTokens = errors.map(_.split(" ")).map(p => p(1)) // take the second column\
\

\b // keep in cache  after the first action
\b0 \
messageTokens.cache()\
\
val mysqlMessages = messageTokens.filter(_.contains("mysql")).count()\
\

> uses cached messageTokens

val javaMessages = messageTokens.filter(_.contaims(\'93java\'94)).count()\
\

 Basic Example
\
val fileRDD1 = sc.textFile(\'93ERROR_DATA.txt")\
/.flatMap(_.split(" "))\
/.filter(_.contains("Spark"))\
/.map(w => (w,1))\
/.reduceByKey(_+_)\
\
val fileRDD2 = sc.textFile(\'93ORIGINAL_DATA.txt\'94).flatMap(_.split(" ")).filter(_.contains("Spark")).map(w => (w,1)).reduceByKey(_+_)\
\
fileRDD1.join(fileRDD2).collect()\
\
res18: Array[(String, (Int, Int))] = Array((Spark,(159,18)))\
\

Concepts :

1. Driver - first serializes the lineage DAG - and sends it to every Worker Node \
     -> also sends out the tasks to execute\
2. Every Worker reads a partition of RDD for each HDFS block and cache the message locally\
3. Performs the count locally and sends back the result to Driver - in parallel\
4. Driver aggregates the counts \
\
DriverManager \ SparkContext  \
- master connects to the 'Cluster Manager' to allocate resources\
- master acquires Executors on cluster nodes (Worker  can run multiple Executor Jvm processes) \
- master sends App Code (Closure Serialization) to the Executors \
- master sends Tasks for the Executor to run ...\
\
prefer re-computation of transformation to leverage memory (part that got lost) as opposed to duplication of data.\
\
Note the difference between Map and FlatMap >>\
\
map(l => l.split(" ")) >>> Array[Array[String]]\
flatMap(l => l.split(" ")) >>> [Array[String]\
\

\b Quick recap of scala and python :\

val pair = (a,b)\
pair._1\
pair._2\
\
pair = (a,b)\
pair[0]\
pair[1]\
\
Tuple2 pair = new Tuple2(a,b)\
\
/////////\
\

\b Running Sample Code :
\b0 \
\

\ SparkPi \


\ ./bin/run-example SparkPi 2 local --conf spark.port.maxRetries=100\

\b0 \cf0 \
\cf4 val count = sc.parallelize(1 to n, 2) => ParallelCollectionRDD\
 	 |.map\{i => val x = 2*random - 1  => transformation :: MapPartitionsRDD\
     | val y = 2*random - 1\
     | if (x*x + y*y < 1) 1 else 0\
     | \}.reduce(_+_) => action\
\
val rdd1 = sc.parallelize(1 to n, 2).map\{i => val x = 2*random - 1  \
     | val y = 2*random - 1\
     | if (x*x + y*y < 1) 1 else 0\
     | \}\cf0 \
\

>> Spark Streaming - Twitter Algebird  -> Count-Min Sketch Algo : count Frequencies
\ ./bin/spark-submit --class "org....TwitterAlgebirdCMS"     --master "local[*]" lib/spark-example-*.jar\

>> Network Word Count
Python >> ./bin/spark-submit nwc.py localhost 9999\
Scala >>  ./bin/run-example org.apache.spark.examples.streaming.NetworkWordCount localhost 9999 \
\
>> Streaming Word Count\

scala> val ssc = new StreamingContext(sc, Seconds(2))\
ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@170937fe\
\
scala> val line = ssc.socketTextStream("localhost", 9999)\
line: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@571b4bf6\
\
scala> val words = line.flatMap(_.split(" "))\
words: org.apache.spark.streaming.dstream.DStream[String] = org.apache.spark.streaming.dstream.FlatMappedDStream@15eaa2ce\
\
scala> val wordCounts = words.map(w => (w,1)).reduceByKey(_+_)\
wordCounts: org.apache.spark.streaming.dstream.DStream[(String, Int)] = org.apache.spark.streaming.dstream.ShuffledDStream@2a59a8fe\
\
scala> wordCounts.print()\
\
scala> ssc.start()\
\

>> more exercises

https://github.com/ceteri/intro_spark/tree/master/bikeshare}
