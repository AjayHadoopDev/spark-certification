// real-time machine-learning and contunous training of data

def compute(input: RDD[String]) = input
	.flatMap(_.split(","))
	.map(_,1)
	.reduceByKey(_+_)
	
featurization(input: RDD[String]): RDD[LabeledPoint] = 

import org.apache.spark.streaming._

val sc = new SparkContext("local[2]", "Simple Streaming App")
val ssc = new StreamingContext(sc, Seconds(2))

val stream = ssc.sockettextStream("localhost", 9999)

stream
	.window(Seconds(30))
	.transform(rdd => {
	 val model = SVMWithSGD.train(featurization(rdd, 12))
	 rdd
	 }) 	
/// computation - should finish within batch window ... 	 
///  ML Lib - not requires a Context

-> StreamingKMeans
-> StreamingLinearRegressionWithSGD

// Spark-Kafka Connection
-> use direct stream handling (one Executor maps to one Kafka Partition)
-> here - no dedicated Receiver

Trouble shooting ::
>> 
Thread 78 : Executor task launch worker-0 (WAITING)

//// Integration of Streaming with SQL Dataframe

val sqlCtx = new SQLContext(sc)

import sqlCtx.implicits._
val ssc = new StreamingContext(sc, Seconds(2))
val stream = ssc.sockettextStream("localhost", 9999)

stream
	.window(Seconds(30))
	.transform(rdd =>  {
		val df = compute(rdd).toDF("word", "count")
		df.describe.().rdd
		})
		.print
...
...

////////////

Twitter Streams >>>
Scrape/Collect data  (Stream)
Clean Explore Feature Engineering Data (SQL)
Build Model(case Class)
Improve Model()
Apply model()

////

https://github.com/data-mining/reference-apps/blob/master/twitter_classifier/scala/src/main/scala/com/databricks/apps/twitter_classifier/Collect.scala

1. extract text from twitter 	
>>

2. sequence text as bigrams 
>> tweet.sliding(2).toseq
("Ce" , "ec" , "ci" , ".....")

3. convert bigrams into numbers
>> seq.mao(_.hashCode())
(2178, 3230, 3174 ....)

4. index into sparse tf vectors

>> seq.mao(_.hashCode() % 1000)

5. Vector.sparse(1000, ....)


/Users/kmanda1/Documents/code/spark/reference-apps-master

cd twitter-classifier
sbt scala/build.sbt

export SPARK_HOME=/Users/kmanda1/Downloads/spark-1.3.0-bin-hadoop2.4/		

////////////

${SPARK_HOME}/bin/spark-submit \
--class "com.databricks.apps.twitter_class	

>> val tweetStream = TwitterUtils.createStream(ssc, Utils.getAuth)
  .map(gson.toJson(_))

>> tweetStream.foreachRDD((rdd, time) => {
  val count = rdd.count() 
  .....
  val outputRDD = rdd.repartition(partitionsEachInterval)
  outputRDD.saveAsTextFile(..)
	 
>> 

	 
