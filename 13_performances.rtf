{\rtf1\ansi\ansicpg1252\cocoartf1265\cocoasubrtf190
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 spark.sql.inMemoryColumnarStorage.compressed : true\
Compress the in-memory columnar storage automatically. Previously defaulted to false.\
\
spark.sql.inMemoryColumnarStorage.batchSize : 1000\
The batch size for columnar caching. Larger values may cause out-of-memory problems\
\
spark.sql.parquet.compression.codec : snappy\
Which compression codec to use. Possible options include uncompressed, snappy, gzip, and lzo.\
\
///////\
\
What minimum batch size Spark Streaming can use ?\
\
In general, 500 milliseconds has proven to be a good minimum size for many applications\
\
The best approach is to start with a larger batch size (around 10 seconds) \
and work your way down to a smaller batch size. \
If the processing times reported in the Streaming UI remain consistent, \
then you can continue to decrease the batch size, \
but if they are increasing you may have reached the limit for your application.\
\
Level of Parallelism :\
Increasing the number of receivers ::\
>> You can add more receivers by creating multiple input DStreams (which creates multiple receivers), \
and then applying union to merge them into a single stream.\
>> If receivers cannot be increased anymore, you can further redistribute the received data \
by explicitly repartitioning the input stream ...\
--> DStream.repartition\
\
spark.executor.extraJavaOptions=-XX:+UseConcMarkSweepGC \
\
Machine Learning Performance :\
> feature preparation is the most important step to large-scale learning\
> Featurize text correctly. Use an external library like NLTK to stem words, \
  and use IDF across a representative corpus for TF-IDF.\
  (IDF - acts as a good feature)\
  \
> MLlib requires class labels to be 0 to C\'961 ...... \
> most of the SGD-based algorithms require around 100 iterations to get good results\
> In Python, MLlib automatically caches RDDs \
\
>  In terms of processing cost, sparse vectors are generally cheaper to compute on \
if at most 10% of the entries are nonzero.}