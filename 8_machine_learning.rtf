{\rtf1\ansi\ansicpg1252\cocoartf1265\cocoasubrtf190
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 SVM\
Logistic\
Linear \
\
K-Means Clustering\
\
///\
import org.apache.spark.SparkConf;\
import org.apache.spark.api.java.JavaRDD;\
import org.apache.spark.api.java.JavaSparkContext;\
import org.apache.spark.sql.DataFrame;\
import org.apache.spark.sql.Row;\
import org.apache.spark.sql.SQLContext;\
import org.apache.spark.sql.types.StructField;\
import org.apache.spark.sql.types.StructType;\
\
object MLLIB extends App \{\
\
val sc = new SparkContext("local","SVM App");\
\
//// SUPERVISED\
\
val path = "/Users/kmanda1/Documents/code/spark_guide/Chapter5/data_titanic.json/"\
\
val sqlContext =  new SQLContext(sc);\
\
val df = sqlContext.load(path, "json")\
\
val persons = df.select("name" , "age", "sex" , "survived").rdd;\
\
val features = persons.map(row => \
LabeledPoint(row.getLong(3).toDouble,  		-->  Y  (0/1 - human annotated labels)\
 Vectors.dense(\
    row.getDouble(1),  						-->  X\
    if (row.getString(2) == "M") 0 else 1	-->  X\
    )\
   )\
  )\
  \
val splits = features.randomSplit(Array(0.6, 0.4))\
val trainingSet = splits(0)\
val validationSet = splits(1)\
\
val model = SVMWithSGD.train(trainingSet, 50)  \
\
val scoreAndLabels = validationSet.map \{point => \
 val score = model.predict(point.features)\
 (score,point.label)\
\}\
\
val metrics = new BinaryClassificationMetrics(scoreAndLabels)\
val auROC = metrics.areaUnderROC()\
\
println("Area under ROC = " + auROC)\
\}\
\
//// UNSUPERVISED\
import org.apache.spark.mllib.linalg.Vectors\
val path = "/Users/kmanda1/Documents/code/spark_guide/Chapter6/iris.data/"\
val sc = new SparkContext("local", "KMeans App");\
\
val data: RDD[org.apache.spark.mllib.linalg.Vector] = sc.textFile(path).map(line => Vectors.dense(\
	line.split(",").slice(0,3).map(_.toDouble))\
	)\
\
val computedCenters: KMeansModel = KMeans.train(data, k=20, maxIterations=50)\
\
val WSSE = computedCenters.computeCost(data)\
\
computedCenters.predict()\
\}\
\
/////////\
import org.apache.spark.mllib.regression.LabeledPoint\
import org.apache.spark.mllib.feature.HashingTF\
import org.apache.spark.mllib.classification.LogisticRegressionWithSGD\
\
val spam = sc.textFile("/Users/kmanda1/Documents/code/spark/learning-spark-master/files/spam.txt")\
\
val normal = sc.textFile("/Users/kmanda1/Documents/code/spark/learning-spark-master/files/ham.txt")\
\
val tf = new HashingTF(numFeatures = 10000)\
val spamFeatures = spam.map(email => tf.transform(email.split(" ")))\
val normalFeatures = normal.map(email => tf.transform(email.split(" ")))\
\
val positiveExamples = spamFeatures.map(features => LabeledPoint(1, features))\
val negativeExamples = normalFeatures.map(features => LabeledPoint(0, features))\
val trainingData = positiveExamples.union(negativeExamples)\
trainingData.cache() // Cache since Logistic Regression is an iterative algorithm.\
\
val model = new LogisticRegressionWithSGD().run(trainingData)\
val posTest = tf.transform("O M G GET cheap stuff by sending money to ...".split(" "))\
val negTest = tf.transform("Hi Dad, I started studying Spark the other ...".split(" "))\
println("Prediction for positive test example: " + model.predict(posTest))\
println("Prediction for negative test example: " + model.predict(negTest))\
\
// val posTest2 = tf.transform("prince Africa money...".split(" "))\
}