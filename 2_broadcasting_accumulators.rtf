Concepts of Broadcast :
\
keep a read-only copy cached - on each machine (say a ML Classifier)\
- rather than shipping a copy of it with Tasks\
- save Network cost (shuffling) through co-located data needed for JOIN\
\
Example :\
\
// Look up the countries for each call_records for the contactCounts RDD.  \
// We load an array of call_record with country code as prefix to support this lookup.\
\
val callRecords = sc.broadcast(loadCallRecordsTable())\
val countryContactCounts = contactCounts.map\{case (call_rec, count) =>\
   // during runtime, lookup the broadcasted data to find \'91country\'92 prefix for a given call_rec \
  val country = lookupInArray(call_rec, callRecords.value)\
  (country, count)\
\}.reduceByKey((x, y) => x + y)\
countryContactCounts.saveAsTextFile(outputDir + "/countries.txt")\
\
/////  Optimize :\
\
When we are broadcasting large values, \
it is important to choose a data serialization format that is both fast and compact, \
because the time to send the value over the network can quickly become a bottleneck \
if it takes a long time to either serialize a value or to send the serialized value over the network. \
In particular, Java Serialization, the default serialization library used in Spark\'92s Scala and Java APIs, \
can be very inefficient out of the box for anything except arrays of primitive types. \
You can optimize serialization by selecting a different serialization library using the spark.serializer property\
\
///////// Accumulator\
\
Only driver program can read accumulator values ..\
\
map() function or a condition for filter(), they can use variables defined outside them \
in the driver program, but each task running on the cluster gets a new copy of each variable, \
and updates from these copies are not propagated back to the driver.\
 \
Spark\'92s shared variables, accumulators and broadcast variables, relax this restriction \
for two common types of communication patterns: aggregation of results and broadcasts.\
\
////////\
\
val data = sc.parallelize(1 to 1000)\
val double = data.map(_*2)\
val accum = sc.accumulator(0)\
double.foreach(accum += _)  || in java ::  double.foreach(val -> accum.add(val))\
accum.value\
\
/// aggregating values from worker nodes back to the driver program\
\
Accumulator empty line count in Python\
file = sc.textFile(inputFile)\
# Create Accumulator[Int] initialized to 0\
blankLines = sc.accumulator(0)\
\
def extractCallSigns(line):\
    global blankLines   # Make the global variable accessible\
    if (line == ""):\
        blankLines += 1\
    return line.split(" ")\
\
callSigns = file.flatMap(extractCallSigns)\
callSigns.saveAsTextFile(outputDir + "/callsigns")\
print "Blank lines: %d" % blankLines.value\
\
/////\
val file = sc.textFile("file.txt")\
\
val blankLines = sc.accumulator(0)  // Create an Accumulator[Int] initialized to 0\
\
val callSigns = file.flatMap(line => \{\
  if (line == "") \{\
    blankLines += 1 // Add to the accumulator\
  \}\
  line.split(" ")\
\})\
\
callSigns.saveAsTextFile("output.txt")\
println("Blank lines: " + blankLines.value)\
\
//////\
\
--> Observe the Execution Flow <--\
SparkContext: Starting job: foreach \
DAGScheduler: Got job 44 with 8 output partitions  || JOB SCHEDULER \
\
	DAGScheduler: Submitting Stage 65 (MapPartitionsRDD\
\
	MemoryStore: ensureFreeSpace ...  || MEMORY STORE\
	Block broadcast_65_piece0 stored as bytes in memory (estimated size 3.5 KB, free 263.7 MB)\
\
	BlockManagerInfo: Added broadcast_65_piece0 in memory on localhost:50656 || DATA BLOCK\
	\
SparkContext: Created broadcast 65 from broadcast\
DAGScheduler: Submitting 8 missing tasks from Stage 65	\
\
>> TaskSchedulerImpl: Adding task set 65.0 with 8 tasks\
>> TaskSetManager: Starting task 0.0 in stage 65.0 (TID 91, localhost, PROCESS_LOCAL, 1260 bytes)\
...... similarly start other 7 tasks\
\
>>>   Executor: Finished task 5.0 in stage 65.0 (TID 96). 705 bytes result sent to driver \
\
SEE THE PERFORMANCE GAIN ::\
>> All tasks in parallel ||\
16/03/08 13:01:44 INFO TaskSetManager: Finished task 2.0 in stage 65.0 (TID 93) in 14 ms on localhost (1/8)\
16/03/08 13:01:44 INFO TaskSetManager: Finished task 7.0 in stage 65.0 (TID 98) in 13 ms on localhost (2/8)\
16/03/08 13:01:44 INFO TaskSetManager: Finished task 4.0 in stage 65.0 (TID 95) in 14 ms on localhost (3/8)\
16/03/08 13:01:44 INFO TaskSetManager: Finished task 6.0 in stage 65.0 (TID 97) in 14 ms on localhost (4/8)\
16/03/08 13:01:44 INFO TaskSetManager: Finished task 3.0 in stage 65.0 (TID 94) in 14 ms on localhost (5/8)\
16/03/08 13:01:44 INFO TaskSetManager: Finished task 5.0 in stage 65.0 (TID 96) in 14 ms on localhost (6/8)\
16/03/08 13:01:44 INFO TaskSetManager: Finished task 1.0 in stage 65.0 (TID 92) in 14 ms on localhost (7/8)\
16/03/08 13:01:44 INFO TaskSetManager: Finished task 0.0 in stage 65.0 (TID 91) in 22 ms on localhost (8/8)\
\
>>>\
16/03/08 13:01:44 INFO TaskSchedulerImpl: Removed TaskSet 65.0, whose tasks have all completed, from pool \
16/03/08 13:01:44 INFO DAGScheduler: Stage 65 (foreach at <console>:28) finished in 0.023 s\
16/03/08 13:01:44 INFO DAGScheduler: Job 44 finished: foreach at <console>:28, took 0.030039 s}
