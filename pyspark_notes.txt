
/// PYSPARK SHELL
 
>>> ./bin/pyspark 
>>> lines.flatMap(lambda line : line.split(" ")).map(lambda word : (word,1)).reduceByKey(lambda current, accumulator : (current+accumulator)).collect()

>>> data = sc.parallelize(list(range(1,1000))).map(lambda x: x**2)
>>> data.first()

>> accum = sc.accumulator(0)
>> data.foreach(lambda x: accum.add(x))
>> accum.value

def accumulateData(x):
	accum.add(x)

data.foreach(accumulateData)

////////////////////////////

sc.broadcast(list(range(1,4)))

///////

import sys
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

def updateFunc(values, last_sum):
	return sum(values) + (last_sum or 0)

sc = SparkContext(appName="PyStreamText", master="local[*]")
ssc = StreamingContext(sc, 5)

ssc.checkpoint("checkpoint")

lines = ssc.socketTextStream("localhost" , 9999)

#counts = lines.flatMap(lambda line : line.split(" ")).map(lambda w: (w,1)).reduceByKey(lambda a,b : a+b)

counts = lines.flatMap(lambda line : line.split(" ")).map(lambda w: (w,1)).updateStateByKey(updateFunc).transform(lambda x: x.sortByKey())

counts.pprint()
ssc.start()
ssc.awaitTermination()


//////  -> Real-tome-streaming with update-state in memory


	